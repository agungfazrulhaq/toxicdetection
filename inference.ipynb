{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2afcd9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150)\n",
      "comment is toxic (93.0254757%)\n",
      "comment is obscene (55.6676269%)\n",
      "comment is an insult (71.0837722%)\n",
      "CPU times: user 149 ms, sys: 28.2 ms, total: 177 ms\n",
      "Wall time: 197 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Test Kserve model inference service for Toxic Detection\n",
    "Input: Text (String)\n",
    "Output: Toxicity Elements\n",
    "\"\"\"\n",
    "\n",
    "from kubernetes import client \n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import utils\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1SKLearnSpec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from minio import Minio\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Removes a list of stopwords\n",
    "\n",
    "    Args:\n",
    "        sentence (string): sentence to remove the stopwords from\n",
    "\n",
    "    Returns:\n",
    "        sentence (string): lowercase sentence without the stopwords\n",
    "    \"\"\"\n",
    "    # List of stopwords\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "    # Sentence converted to lowercase-only\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    words = sentence.split()\n",
    "    no_words = [w for w in words if w not in stopwords]\n",
    "    sentence = \" \".join(no_words)\n",
    "\n",
    "    return sentence\n",
    "    \n",
    "def remove_symbols(sentence) :\n",
    "    return re.sub(r'[^\\w]', ' ', sentence)\n",
    "\n",
    "\n",
    "minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "minio_bucket = \"mlpipeline\"\n",
    "\n",
    "EMBEDDING_DIM = 16\n",
    "PADDING = 'post'\n",
    "TRUNCATING = 'post'\n",
    "MAXLEN = 150\n",
    "    \n",
    "minio_client.fget_object(minio_bucket, \"commentoxic/tokenizer.pickle\", \"/tmp/tokenizer.pickle\")\n",
    "file = open('/tmp/tokenizer.pickle', 'rb')\n",
    "tokenizer = pickle.load(file)\n",
    "\n",
    "text = \"you dont deserve to live cause you are a mexican and i hope you dead you nigger mexican\"\n",
    "text_after_stopwords = remove_stopwords(text)\n",
    "text_clean = remove_symbols(text_after_stopwords)\n",
    "\n",
    "text_sequence = tokenizer.texts_to_sequences([text_clean])\n",
    "text_padded = pad_sequences(text_sequence, maxlen=MAXLEN, padding=PADDING, truncating=TRUNCATING)\n",
    "\n",
    "labels = ['toxic','severe toxic','obscene','a threat','an insult','identity hate']\n",
    "KServe = KServeClient()\n",
    "\n",
    "isvc_resp = KServe.get(\"toxic-comment-2023-02-23--08-01-38\", namespace=\"researchai\")\n",
    "isvc_url = isvc_resp['status']['address']['url']\n",
    "\n",
    "t = np.array(text_padded)\n",
    "print(t.shape)\n",
    "# t = t.reshape(-1,28,28,1)\n",
    "\n",
    "inference_input = {\n",
    "  'instances': t.tolist()\n",
    "}\n",
    "\n",
    "response = requests.post(isvc_url, json=inference_input)\n",
    "r = json.loads(response.text)\n",
    "predicted = r['predictions'][0]\n",
    "# predicted = model.predict(text_padded)[0]\n",
    "iter_ = 0\n",
    "for lab in labels :\n",
    "    if predicted[iter_] > 0.5 :\n",
    "        print(\"comment is \"+lab+\" (\"+str(predicted[iter_]*100)+\"%)\")\n",
    "    iter_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88dd49fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.859039485,\n",
       "   0.00670921803,\n",
       "   0.0814264417,\n",
       "   0.0786735713,\n",
       "   0.474647522,\n",
       "   0.191876709]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fae39eda",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (249141294.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [20]\u001b[0;36m\u001b[0m\n\u001b[0;31m    '{ \"error\": \"NodeDef mentions attr \\\\\\'parallel_iterations\\\\\\' not in Op<name=StatelessWhile; signature=input: -> output:; attr=T:list(type),min=0; attr=cond:func; attr=body:func>; NodeDef: {{node StatefulPartitionedCall/StatefulPartitionedCall/sequential/bidirectional/backward_lstm/PartitionedCall/while}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\\\\n\\\\t [[StatefulPartitionedCall/StatefulPartitionedCall/sequential/bidirectional/backward_lstm/PartitionedCall/while]]\"\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "'{ \"error\": \"NodeDef mentions attr \\\\\\'parallel_iterations\\\\\\' not in Op<name=StatelessWhile; signature=input: -> output:; attr=T:list(type),min=0; attr=cond:func; attr=body:func>; NodeDef: {{node StatefulPartitionedCall/StatefulPartitionedCall/sequential/bidirectional/backward_lstm/PartitionedCall/while}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\\\\n\\\\t [[StatefulPartitionedCall/StatefulPartitionedCall/sequential/bidirectional/backward_lstm/PartitionedCall/while]]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99f110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
