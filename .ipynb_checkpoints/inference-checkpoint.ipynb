{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecbd23f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:63\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Test Kserve model inference service for Toxic Detection\n",
    "Input: Text (String)\n",
    "Output: Toxicity Elements\n",
    "\"\"\"\n",
    "\n",
    "from kubernetes import client \n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import utils\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1SKLearnSpec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from minio import Minio\n",
    "import pickle\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Removes a list of stopwords\n",
    "\n",
    "    Args:\n",
    "        sentence (string): sentence to remove the stopwords from\n",
    "\n",
    "    Returns:\n",
    "        sentence (string): lowercase sentence without the stopwords\n",
    "    \"\"\"\n",
    "    # List of stopwords\n",
    "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "    # Sentence converted to lowercase-only\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    words = sentence.split()\n",
    "    no_words = [w for w in words if w not in stopwords]\n",
    "    sentence = \" \".join(no_words)\n",
    "\n",
    "    return sentence\n",
    "    \n",
    "def remove_symbols(sentence) :\n",
    "    return re.sub(r'[^\\w]', ' ', sentence)\n",
    "\n",
    "\n",
    "minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "minio_bucket = \"mlpipeline\"\n",
    "\n",
    "EMBEDDING_DIM = 16\n",
    "PADDING = 'post'\n",
    "TRUNCATING = 'post'\n",
    "MAXLEN = 150\n",
    "    \n",
    "minio_client.fget_object(minio_bucket, \"commentoxic/tokenizer.pickle\", \"/tmp/tokenizer.pickle\")\n",
    "file = open('/tmp/tokenizer.pickle', 'rb')\n",
    "tokenizer = pickle.load(file)\n",
    "\n",
    "text = \"i hate you and i hope the worst for you\"\n",
    "text_after_stopwords = remove_stopwords(text)\n",
    "text_clean = remove_symbols(text_after_stopwords)\n",
    "\n",
    "text_sequence = tokenizer.texts_to_sequences([text_clean])\n",
    "text_padded = pad_sequences(text_sequence, maxlen=MAXLEN, padding=PADDING, truncating=TRUNCATING)\n",
    "\n",
    "labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "predicted = model.predict(text_padded)[0]\n",
    "iter_ = 0\n",
    "for lab in labels :\n",
    "    print(predicted[iter_])\n",
    "    if predicted[iter_] > 0.5 :\n",
    "        print(\"comment contains \"+lab)\n",
    "    iter_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8c4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
