{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc35743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f17979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def download_and_load_data():\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    filename = \"toxic_comments.csv\"\n",
    "    try:\n",
    "        response = minio_client.get_object(minio_bucket, \"dataset/\"+filename)\n",
    "        # Read data from response.\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "    except:\n",
    "        url = \"https://raw.githubusercontent.com/agungfazrulhaq/toxicdetection/main/data/train.csv\"\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(filename, 'wb').write(r.content)\n",
    "\n",
    "        print(\"Downloaded file \"+filename)\n",
    "        minio_client.fput_object(minio_bucket, \"dataset/toxic_comments.csv\", \"toxic_comments.csv\")\n",
    "        print(\"Stored downloaded dataset\")\n",
    "\n",
    "def transform_and_split_train(split_size:float = 0.9) :\n",
    "    import tensorflow as tf\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    filename = \"toxic_comments.csv\"\n",
    "    def remove_stopwords(sentence):\n",
    "        \"\"\"\n",
    "        Removes a list of stopwords\n",
    "\n",
    "        Args:\n",
    "            sentence (string): sentence to remove the stopwords from\n",
    "\n",
    "        Returns:\n",
    "            sentence (string): lowercase sentence without the stopwords\n",
    "        \"\"\"\n",
    "        # List of stopwords\n",
    "        stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "        # Sentence converted to lowercase-only\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        words = sentence.split()\n",
    "        no_words = [w for w in words if w not in stopwords]\n",
    "        sentence = \" \".join(no_words)\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    def remove_symbols(sentence) :\n",
    "        return re.sub(r'[^\\w]', ' ', sentence)\n",
    "    \n",
    "    minio_client.fget_object(minio_object, \"dataset/toxic_comments.csv\", \"/tmp/toxic_comments.csv\")\n",
    "    df_train = pd.read_csv(\"toxic_comments.csv\")\n",
    "    train = df_train[['comment_text','toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "    \n",
    "    train[\"text_no_stopwords\"] = train[\"comment_text\"].apply(lambda x : remove_stopwords(x))\n",
    "    train[\"text_final\"] = train[\"text_no_stopwords\"].apply(lambda x : remove_symbols(x))\n",
    "    \n",
    "    def train_val_split(data, split) :\n",
    "        train = []\n",
    "        train_label = []\n",
    "        validation = []\n",
    "        val_label = []\n",
    "        for ind,val in data.iterrows() :\n",
    "            if len(train) < len(data)*split :\n",
    "                train.append(val['text_final'])\n",
    "                train_label.append(np.array(val[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values))\n",
    "            else :\n",
    "                validation.append(val['text_final'])\n",
    "                val_label.append(np.array(val[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values))\n",
    "\n",
    "        train = np.array(train)\n",
    "        train_label = np.array(train_label)\n",
    "        validation = np.array(validation)\n",
    "        val_label = np.array(val_label)\n",
    "\n",
    "        return train, train_label, validation, val_label\n",
    "    \n",
    "    x_train, y_train, x_val, y_val = train_val_split(train, split_size)\n",
    "    \n",
    "    y_train = np.asarray(y_train).astype(np.float32)\n",
    "    y_val = np.asarray(y_val).astype(np.float32)\n",
    "    \n",
    "    np.save(\"/tmp/x_train.npy\", x_train)\n",
    "    minio_client.fput_object(minio_object, \"commentoxic/x_train\", \"/tmp/x_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/y_train.npy\", y_train)\n",
    "    minio_client.fput_object(minio_object, \"commentoxic/y_train\", \"/tmp/y_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/x_val.npy\", x_val)\n",
    "    minio_client.fput_object(minio_object, \"commentoxic/x_val\", \"/tmp/x_val.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/y_val.npy\", y_val)\n",
    "    minio_client.fput_object(minio_object, \"commentoxic/y_val\", \"/tmp/y_val.npy\")\n",
    "    \n",
    "    print(\"Train shape :\", x_train.shape)\n",
    "    print(\"Validation shape :\", x_val.shape)\n",
    "\n",
    "def building_tokenizer(oov_token:str = \"<OOV>\"):\n",
    "    \n",
    "    from minio import Minio\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.utils import to_categorical \n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    def fit_tokenizer(train_sentences, oov_token) :\n",
    "        tokenizer = Tokenizer(oov_token=oov_token)\n",
    "        tokenizer.fit_on_texts(train_sentences)\n",
    "    \n",
    "        return tokenizer\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"commentoxic/x_train\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    tokenizer = fit_tokenizer(x_train, oov_token)\n",
    "    \n",
    "    with open('tmp/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    minio_client.fput_object(minio_bucket,\"commentoxic/tokenizer.pickle\", \"/tmp/tokenizer.pickle\")\n",
    "    \n",
    "    \n",
    "def building_model(n_epochs:int = 10,\n",
    "                   optimizer:str = \"adam\",\n",
    "                   max_len:int = 120) -> NamedTuple('Output', [('mlpipeline_metrics', 'Metrics')]):\n",
    "    from minio import Minio\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.utils import to_categorical \n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    EMBEDDING_DIM = 16\n",
    "    PADDING = 'post'\n",
    "    TRUNCATING = 'post'\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    NUM_WORDS = len(word_index)\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length=max_len),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(6, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Print the model summary\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/tokenizer.pickle\", \"/tmp/tokenizer.pickle\")\n",
    "    file = open('/tmp/tokenizer.pickle', 'rb')\n",
    "    tokenizer = pickle.load(file)\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/x_train\", \"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/y_train\", \"/tmp/y_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/x_val\", \"/tmp/x_val.npy\")\n",
    "    x_val = np.load(\"/tmp/x_val.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/y_val\", \"/tmp/y_val.npy\")\n",
    "    y_val = np.load(\"/tmp/y_val.npy\")\n",
    "    \n",
    "\n",
    "    training_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=PADDING, truncating=TRUNCATING)\n",
    "\n",
    "    validation_sequences = tokenizer.texts_to_sequences(x_val)\n",
    "    validation_padded = pad_sequences(validation_sequences, maxlen=max_len, padding=PADDING, truncating=TRUNCATING)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(training_padded, y_train, epochs=num_epochs, validation_data=(validation_padded, y_val))\n",
    "    \n",
    "    model.save(\"/tmp/toxicmodel\")\n",
    "    model.save(\"/tmp/toxic_model.h5\")\n",
    "    \n",
    "    minio_client.fput_object(minio_object, \"commentoxic/toxic_model.h5\", \"/tmp/toxic_model.h5\")\n",
    "    \n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "    \n",
    "    upload_local_directory_to_minio(\"/tmp/toxicmodel\", minio_bucket, \"commentoxic/model/toxicmodel/1/\")\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(history.history[\"accuracy\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(history.history[\"loss\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_val_accuracy',\n",
    "          'numberValue':  float(history.history[\"val_accuracy\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_val_loss',\n",
    "          'numberValue':  float(history.history[\"val_loss\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_metrics'])\n",
    "    return output(json.dumps(metrics))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3476fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
