{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69902cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874e3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<object object at 0x7f8a54427190>\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "\n",
    "print(urllib3.Timeout.DEFAULT_TIMEOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da4d15a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/62a84ecf-17e7-40c5-8d96-fa2485d65bfb\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/420fe435-85b3-4188-b431-37200b047fe7\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def download_and_load_data():\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    import urllib3\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False,\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    filename = \"toxic_comments.csv\"\n",
    "    try:\n",
    "        response = minio_client.get_object(minio_bucket, \"dataset/\"+filename)\n",
    "    except:\n",
    "        url = \"https://raw.githubusercontent.com/agungfazrulhaq/toxicdetection/main/data/train.csv\"\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(filename, 'wb').write(r.content)\n",
    "\n",
    "        print(\"Downloaded file \"+filename)\n",
    "        minio_client.fput_object(minio_bucket, \"dataset/toxic_comments.csv\", \"toxic_comments.csv\")\n",
    "        print(\"Stored downloaded dataset\")\n",
    "\n",
    "def transform_and_split_train() :\n",
    "    import tensorflow as tf\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import re\n",
    "    import os\n",
    "    \n",
    "    filename = \"toxic_comments.csv\"\n",
    "    def remove_stopwords(sentence):\n",
    "        \"\"\"\n",
    "        Removes a list of stopwords\n",
    "\n",
    "        Args:\n",
    "            sentence (string): sentence to remove the stopwords from\n",
    "\n",
    "        Returns:\n",
    "            sentence (string): lowercase sentence without the stopwords\n",
    "        \"\"\"\n",
    "        # List of stopwords\n",
    "        stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "        # Sentence converted to lowercase-only\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        words = sentence.split()\n",
    "        no_words = [w for w in words if w not in stopwords]\n",
    "        sentence = \" \".join(no_words)\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    def remove_symbols(sentence) :\n",
    "        return re.sub(r'[^\\w]', ' ', sentence)\n",
    "    \n",
    "    import urllib3\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False,\n",
    "        http_client=urllib3.ProxyManager(\n",
    "            \"http://192.168.1.10:30950\",\n",
    "            timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n",
    "            # cert_reqs=\"CERT_REQUIRED\",\n",
    "            retries=urllib3.Retry(\n",
    "                total=5,\n",
    "                backoff_factor=0.2,\n",
    "                status_forcelist=[500, 502, 503, 504],\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"dataset/toxic_comments.csv\", \"/tmp/toxic_comments.csv\")\n",
    "    df_train = pd.read_csv(\"/tmp/toxic_comments.csv\")\n",
    "    train = df_train[['comment_text','toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "    \n",
    "    train[\"text_no_stopwords\"] = train[\"comment_text\"].apply(lambda x : remove_stopwords(x))\n",
    "    train[\"text_final\"] = train[\"text_no_stopwords\"].apply(lambda x : remove_symbols(x))\n",
    "    \n",
    "    split_size=0.9\n",
    "    def train_val_split(data, split) :\n",
    "        train = []\n",
    "        train_label = []\n",
    "        validation = []\n",
    "        val_label = []\n",
    "        for ind,val in data.iterrows() :\n",
    "            if len(train) < len(data)*split :\n",
    "                train.append(val['text_final'])\n",
    "                train_label.append(np.array(val[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values))\n",
    "            else :\n",
    "                validation.append(val['text_final'])\n",
    "                val_label.append(np.array(val[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values))\n",
    "\n",
    "        train = np.array(train)\n",
    "        train_label = np.array(train_label)\n",
    "        validation = np.array(validation)\n",
    "        val_label = np.array(val_label)\n",
    "\n",
    "        return train, train_label, validation, val_label\n",
    "    \n",
    "    x_train, y_train, x_val, y_val = train_val_split(train, split_size)\n",
    "    \n",
    "    y_train = np.asarray(y_train).astype(np.float32)\n",
    "    y_val = np.asarray(y_val).astype(np.float32)\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    batch = 50\n",
    "    sliced_batch = math.ceil(x_train.shape[0]/batch)\n",
    "    for i in range(1, batch+1) :\n",
    "        filename = \"/tmp/x_train_\"+str(i)+\".npy\"\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            if i == batch :\n",
    "                np.save(f, np.array(x_train[((i-1)*sliced_batch):]))\n",
    "            else:\n",
    "                np.save(f, np.array(x_train[((i-1)*sliced_batch):(i*sliced_batch)]))\n",
    "            minio_client.fput_object(minio_bucket, \"commentoxic/x_train_\"+str(i), filename)\n",
    "            print(\"saved \"+filename)\n",
    "            \n",
    "#     np.save(\"/tmp/x_train.npy\", x_train)\n",
    "#     file_size = os.path.getsize('/tmp/x_train.npy')\n",
    "#     print(\"File Size is :\", file_size, \"bytes\")\n",
    "    \n",
    "    \n",
    "    np.save(\"/tmp/y_train.npy\", y_train)\n",
    "    minio_client.fput_object(minio_bucket, \"commentoxic/y_train\", \"/tmp/y_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/x_val.npy\", x_val)\n",
    "    minio_client.fput_object(minio_bucket, \"commentoxic/x_val\", \"/tmp/x_val.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/y_val.npy\", y_val)\n",
    "    minio_client.fput_object(minio_bucket, \"commentoxic/y_val\", \"/tmp/y_val.npy\")\n",
    "    \n",
    "    print(\"Train shape :\", x_train.shape)\n",
    "    print(\"Validation shape :\", x_val.shape)\n",
    "\n",
    "def building_tokenizer(oov_token:str = \"<OOV>\"):\n",
    "    \n",
    "    from minio import Minio\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.utils import to_categorical \n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    def fit_tokenizer(train_sentences, oov_token) :\n",
    "        tokenizer = Tokenizer(oov_token=oov_token)\n",
    "        tokenizer.fit_on_texts(train_sentences)\n",
    "    \n",
    "        return tokenizer\n",
    "    \n",
    "    x_train_ = []\n",
    "    batch = 50\n",
    "    for i in range(batch):\n",
    "        minio_client.fget_object(minio_bucket,\"commentoxic/x_train_\"+str(i+1) ,\"/tmp/x_train_\"+str(i+1)+\".npy\")\n",
    "        x_train_.append(np.load(\"/tmp/x_train_\"+str(i+1)+\".npy\"))\n",
    "    \n",
    "    x_train = np.concatenate(x_train_, axis=0)\n",
    "    \n",
    "    tokenizer = fit_tokenizer(x_train, oov_token)\n",
    "    \n",
    "    with open('/tmp/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    minio_client.fput_object(minio_bucket,\"commentoxic/tokenizer.pickle\", \"/tmp/tokenizer.pickle\")\n",
    "    \n",
    "    \n",
    "def building_model(n_epochs:int = 10,\n",
    "                   optimizer:str = \"adam\",\n",
    "                   max_len:int = 120) -> NamedTuple('Output', [('mlpipeline_metrics', 'Metrics')]):\n",
    "    from minio import Minio\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.utils import to_categorical \n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "    import json\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    EMBEDDING_DIM = 16\n",
    "    PADDING = 'post'\n",
    "    TRUNCATING = 'post'\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/tokenizer.pickle\", \"/tmp/tokenizer.pickle\")\n",
    "    file = open('/tmp/tokenizer.pickle', 'rb')\n",
    "    tokenizer = pickle.load(file)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    NUM_WORDS = len(word_index)\n",
    "    \n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length=max_len),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(6, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Print the model summary\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    x_train_ = []\n",
    "    batch = 50\n",
    "    for i in range(batch):\n",
    "        minio_client.fget_object(minio_bucket,\"commentoxic/x_train_\"+str(i+1) ,\"/tmp/x_train_\"+str(i+1)+\".npy\")\n",
    "        x_train_.append(np.load(\"/tmp/x_train_\"+str(i+1)+\".npy\"))\n",
    "    \n",
    "    x_train = np.concatenate(x_train_, axis=0)\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/y_train\", \"/tmp/y_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/x_val\", \"/tmp/x_val.npy\")\n",
    "    x_val = np.load(\"/tmp/x_val.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/y_val\", \"/tmp/y_val.npy\")\n",
    "    y_val = np.load(\"/tmp/y_val.npy\")\n",
    "    \n",
    "\n",
    "    training_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=PADDING, truncating=TRUNCATING)\n",
    "\n",
    "    validation_sequences = tokenizer.texts_to_sequences(x_val)\n",
    "    validation_padded = pad_sequences(validation_sequences, maxlen=max_len, padding=PADDING, truncating=TRUNCATING)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(training_padded, y_train, epochs=n_epochs, validation_data=(validation_padded, y_val))\n",
    "    \n",
    "    model.save(\"/tmp/toxicmodel\")\n",
    "    model.save(\"/tmp/toxic_model.h5\")\n",
    "    \n",
    "    minio_client.fput_object(minio_bucket, \"commentoxic/toxic_model.h5\", \"/tmp/toxic_model.h5\")\n",
    "    \n",
    "    import glob\n",
    "    \n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "    \n",
    "    upload_local_directory_to_minio(\"/tmp/toxicmodel\", minio_bucket, \"commentoxic/model/toxicmodel/1/\")\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(history.history[\"accuracy\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(history.history[\"loss\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_val_accuracy',\n",
    "          'numberValue':  float(history.history[\"val_accuracy\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_val_loss',\n",
    "          'numberValue':  float(history.history[\"val_loss\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_metrics'])\n",
    "    return output(json.dumps(metrics))\n",
    "    \n",
    "def model_serving():\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "    \n",
    "    namespace = utils.get_default_target_namespace()\n",
    "\n",
    "    now = datetime.now()\n",
    "    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "    \n",
    "    name='toxic-comment-{}'.format(v)\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"sa-minio-kserve\",\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=\"s3://mlpipeline/commentoxic/model/toxicmodel/\"))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)\n",
    "\n",
    "comp_get_data = components.create_component_from_func(download_and_load_data,base_image=\"epsindo.ai/tensorflow:22.03-pipeline\")\n",
    "comp_split_train_data = components.create_component_from_func(transform_and_split_train,base_image=\"epsindo.ai/tensorflow:22.03-pipeline\")\n",
    "comp_build_tokenizer = components.create_component_from_func(building_tokenizer,base_image=\"epsindo.ai/tensorflow:22.03-pipeline\")\n",
    "comp_model_building = components.create_component_from_func(building_model,base_image=\"epsindo.ai/tensorflow:22.03-pipeline\")\n",
    "comp_model_serving = components.create_component_from_func(model_serving,base_image=\"epsindo.ai/tensorflow:22.03-pipeline\",\n",
    "                                                           packages_to_install=['kserve==0.7.0'])\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='toxic-comment-pipeline',\n",
    "    description='Detect toxicity of a comment'\n",
    ")\n",
    "def output_test(oov_token,n_epochs,optimizer,max_len):\n",
    "    step1 = comp_get_data()\n",
    "    \n",
    "    step2 = comp_split_train_data()\n",
    "    step2.after(step1)\n",
    "    \n",
    "    step3 = comp_build_tokenizer(oov_token)\n",
    "    step3.after(step2)\n",
    "    \n",
    "    step4 = comp_model_building(n_epochs,optimizer,max_len)\n",
    "    step4.set_gpu_limit(1, 'nvidia')\n",
    "    step4.after(step3)\n",
    "    \n",
    "    step5 = comp_model_serving()\n",
    "    step5.after(step4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = kfp.Client()\n",
    "\n",
    "    arguments = {\n",
    "        \"oov_token\":\"<OOV>\",\n",
    "        \"n_epochs\" : 10,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"max_len\": 150,\n",
    "    }\n",
    "\n",
    "    run_directly = 1\n",
    "    \n",
    "    if (run_directly == 1):\n",
    "        client.create_run_from_pipeline_func(output_test,arguments=arguments,experiment_name=\"test\")\n",
    "    else:\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=output_test,package_path='output_test.yaml')\n",
    "        client.upload_pipeline_version(pipeline_package_path='output_test.yaml',pipeline_version_name=\"0.4\",pipeline_name=\"pipeline test\",description=\"just for testing\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee36dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
