{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfe6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be17cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<object object at 0x7f8a54427190>\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "\n",
    "print(urllib3.Timeout.DEFAULT_TIMEOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "531767ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def download_and_load_data():\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    import urllib3\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False,\n",
    "        http_client=urllib3.ProxyManager(\n",
    "            \"http://192.168.2.193:9000\",\n",
    "            timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n",
    "            # cert_reqs=\"CERT_REQUIRED\",\n",
    "            retries=urllib3.Retry(\n",
    "                total=5,\n",
    "                backoff_factor=0.2,\n",
    "                status_forcelist=[500, 502, 503, 504],\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    filename = \"toxic_comments.csv\"\n",
    "    try:\n",
    "        response = minio_client.get_object(minio_bucket, \"dataset/\"+filename)\n",
    "    except:\n",
    "        url = \"https://raw.githubusercontent.com/agungfazrulhaq/toxicdetection/main/data/train.csv\"\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(filename, 'wb').write(r.content)\n",
    "\n",
    "        print(\"Downloaded file \"+filename)\n",
    "        minio_client.fput_object(minio_bucket, \"dataset/toxic_comments.csv\", \"toxic_comments.csv\")\n",
    "        print(\"Stored downloaded dataset\")\n",
    "\n",
    "def transform_and_split_train() :\n",
    "    import tensorflow as tf\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import re\n",
    "    import os\n",
    "    \n",
    "    filename = \"toxic_comments.csv\"\n",
    "    def remove_stopwords(sentence):\n",
    "        \"\"\"\n",
    "        Removes a list of stopwords\n",
    "\n",
    "        Args:\n",
    "            sentence (string): sentence to remove the stopwords from\n",
    "\n",
    "        Returns:\n",
    "            sentence (string): lowercase sentence without the stopwords\n",
    "        \"\"\"\n",
    "        # List of stopwords\n",
    "        stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "        # Sentence converted to lowercase-only\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        words = sentence.split()\n",
    "        no_words = [w for w in words if w not in stopwords]\n",
    "        sentence = \" \".join(no_words)\n",
    "\n",
    "        return sentence\n",
    "\n",
    "    def remove_symbols(sentence) :\n",
    "        return re.sub(r'[^\\w]', ' ', sentence)\n",
    "    \n",
    "    import urllib3\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False,\n",
    "        http_client=urllib3.ProxyManager(\n",
    "            \"http://192.168.1.10:30950\",\n",
    "            timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n",
    "            # cert_reqs=\"CERT_REQUIRED\",\n",
    "            retries=urllib3.Retry(\n",
    "                total=5,\n",
    "                backoff_factor=0.2,\n",
    "                status_forcelist=[500, 502, 503, 504],\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"dataset/toxic_comments.csv\", \"/tmp/toxic_comments.csv\")\n",
    "    df_train = pd.read_csv(\"/tmp/toxic_comments.csv\")\n",
    "    train = df_train[['comment_text','toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "    \n",
    "    train[\"text_no_stopwords\"] = train[\"comment_text\"].apply(lambda x : remove_stopwords(x))\n",
    "    train[\"text_final\"] = train[\"text_no_stopwords\"].apply(lambda x : remove_symbols(x))\n",
    "    \n",
    "    split_size=0.9\n",
    "    def train_val_split(data, split) :\n",
    "        train = []\n",
    "        train_label = []\n",
    "        validation = []\n",
    "        val_label = []\n",
    "        for ind,val in data.iterrows() :\n",
    "            if len(train) < len(data)*split :\n",
    "                train.append(val['text_final'])\n",
    "                train_label.append(np.array(val[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values))\n",
    "            else :\n",
    "                validation.append(val['text_final'])\n",
    "                val_label.append(np.array(val[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values))\n",
    "\n",
    "        train = np.array(train)\n",
    "        train_label = np.array(train_label)\n",
    "        validation = np.array(validation)\n",
    "        val_label = np.array(val_label)\n",
    "\n",
    "        return train, train_label, validation, val_label\n",
    "    \n",
    "    x_train, y_train, x_val, y_val = train_val_split(train, split_size)\n",
    "    \n",
    "    y_train = np.asarray(y_train).astype(np.float32)\n",
    "    y_val = np.asarray(y_val).astype(np.float32)\n",
    "    \n",
    "    np.save(\"/tmp/x_train.npy\", x_train)\n",
    "\n",
    "    file_size = os.path.getsize('/tmp/x_train.npy')\n",
    "    print(\"File Size is :\", file_size, \"bytes\")\n",
    "    \n",
    "    minio_client.fput_object(minio_bucket, \"commentoxic/x_train\", \"/tmp/x_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/y_train.npy\", y_train)\n",
    "    minio_client.fput_object(minio_bucket, \"commentoxic/y_train\", \"/tmp/y_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/x_val.npy\", x_val)\n",
    "    minio_client.fput_object(minio_bucket, \"commentoxic/x_val\", \"/tmp/x_val.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/y_val.npy\", y_val)\n",
    "    minio_client.fput_object(minio_bucket, \"commentoxic/y_val\", \"/tmp/y_val.npy\")\n",
    "    \n",
    "    print(\"Train shape :\", x_train.shape)\n",
    "    print(\"Validation shape :\", x_val.shape)\n",
    "\n",
    "def building_tokenizer(oov_token:str = \"<OOV>\"):\n",
    "    \n",
    "    from minio import Minio\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.utils import to_categorical \n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    def fit_tokenizer(train_sentences, oov_token) :\n",
    "        tokenizer = Tokenizer(oov_token=oov_token)\n",
    "        tokenizer.fit_on_texts(train_sentences)\n",
    "    \n",
    "        return tokenizer\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"commentoxic/x_train\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    tokenizer = fit_tokenizer(x_train, oov_token)\n",
    "    \n",
    "    with open('tmp/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    minio_client.fput_object(minio_bucket,\"commentoxic/tokenizer.pickle\", \"/tmp/tokenizer.pickle\")\n",
    "    \n",
    "    \n",
    "def building_model(n_epochs:int = 10,\n",
    "                   optimizer:str = \"adam\",\n",
    "                   max_len:int = 120) -> NamedTuple('Output', [('mlpipeline_metrics', 'Metrics')]):\n",
    "    from minio import Minio\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.utils import to_categorical \n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        \"192.168.1.10:30950\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    EMBEDDING_DIM = 16\n",
    "    PADDING = 'post'\n",
    "    TRUNCATING = 'post'\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    NUM_WORDS = len(word_index)\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length=max_len),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(6, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Print the model summary\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/tokenizer.pickle\", \"/tmp/tokenizer.pickle\")\n",
    "    file = open('/tmp/tokenizer.pickle', 'rb')\n",
    "    tokenizer = pickle.load(file)\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/x_train\", \"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/y_train\", \"/tmp/y_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/x_val\", \"/tmp/x_val.npy\")\n",
    "    x_val = np.load(\"/tmp/x_val.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket, \"commentoxic/y_val\", \"/tmp/y_val.npy\")\n",
    "    y_val = np.load(\"/tmp/y_val.npy\")\n",
    "    \n",
    "\n",
    "    training_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=PADDING, truncating=TRUNCATING)\n",
    "\n",
    "    validation_sequences = tokenizer.texts_to_sequences(x_val)\n",
    "    validation_padded = pad_sequences(validation_sequences, maxlen=max_len, padding=PADDING, truncating=TRUNCATING)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(training_padded, y_train, epochs=num_epochs, validation_data=(validation_padded, y_val))\n",
    "    \n",
    "    model.save(\"/tmp/toxicmodel\")\n",
    "    model.save(\"/tmp/toxic_model.h5\")\n",
    "    \n",
    "    minio_client.fput_object(minio_object, \"commentoxic/toxic_model.h5\", \"/tmp/toxic_model.h5\")\n",
    "    \n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "    \n",
    "    upload_local_directory_to_minio(\"/tmp/toxicmodel\", minio_bucket, \"commentoxic/model/toxicmodel/1/\")\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(history.history[\"accuracy\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(history.history[\"loss\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_val_accuracy',\n",
    "          'numberValue':  float(history.history[\"val_accuracy\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_val_loss',\n",
    "          'numberValue':  float(history.history[\"val_loss\"][-1]),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_metrics'])\n",
    "    return output(json.dumps(metrics))\n",
    "    \n",
    "def model_serving():\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "    \n",
    "    namespace = utils.get_default_target_namespace()\n",
    "\n",
    "    now = datetime.now()\n",
    "    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "    \n",
    "    name='toxic-comment-{}'.format(v)\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"sa-minio-kserve\",\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=\"s3://mlpipeline/models/toxicmodel/\"))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)\n",
    "\n",
    "# comp_get_data = components.create_component_from_func(download_and_load_data,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\")\n",
    "# comp_split_train_data = components.create_component_from_func(transform_and_split_train,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\")\n",
    "# comp_build_tokenizer = components.create_component_from_func(building_tokenizer,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\")\n",
    "# comp_model_building = components.create_component_from_func(building_model,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\")\n",
    "# comp_model_serving = components.create_component_from_func(model_serving,base_image=\"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\",\n",
    "#                                                            packages_to_install=['kserve==0.8.0.1'])\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='toxic-comment-pipeline',\n",
    "    description='Detect toxicity of a comment'\n",
    ")\n",
    "def output_test(oov_token,n_epochs,optimizer,max_len):\n",
    "    step1 = comp_get_data()\n",
    "    \n",
    "    step2 = comp_split_train_data()\n",
    "    step2.after(step1)\n",
    "    \n",
    "    step3 = comp_build_tokenizer(oov_token)\n",
    "    step3.after(step2)\n",
    "    \n",
    "    step4 = comp_model_building(n_epochs,optimizer,max_len)\n",
    "    step4.after(step3)\n",
    "    \n",
    "    step5 = comp_model_serving()\n",
    "    step5.after(step4)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     client = kfp.Client()\n",
    "\n",
    "#     arguments = {\n",
    "#         \"oov_token\":\"<OOV>\",\n",
    "#         \"n_epochs\" : 1,\n",
    "#         \"optimizer\": \"adam\",\n",
    "#         \"max_len\": 150,\n",
    "#     }\n",
    "\n",
    "#     run_directly = 1\n",
    "    \n",
    "#     if (run_directly == 1):\n",
    "#         client.create_run_from_pipeline_func(output_test,arguments=arguments,experiment_name=\"test\")\n",
    "#     else:\n",
    "#         kfp.compiler.Compiler().compile(pipeline_func=output_test,package_path='output_test.yaml')\n",
    "#         client.upload_pipeline_version(pipeline_package_path='output_test.yaml',pipeline_version_name=\"0.4\",pipeline_name=\"pipeline test\",description=\"just for testing\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5fd555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size is : 2872280128 bytes\n"
     ]
    },
    {
     "ename": "S3Error",
     "evalue": "S3 operation failed; code: InvalidPart, message: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag., resource: /mlpipeline/commentoxic/x_train, request_id: 1745CACBF8162ABC, host_id: 6810e5c8-b975-4ef0-b5bb-49a4f57b0a8c, bucket_name: mlpipeline, object_name: commentoxic/x_train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mS3Error\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransform_and_split_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtransform_and_split_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m file_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp/x_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile Size is :\u001b[39m\u001b[38;5;124m\"\u001b[39m, file_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m \u001b[43mminio_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminio_bucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcommentoxic/x_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/x_train.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/y_train.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train)\n\u001b[1;32m    135\u001b[0m minio_client\u001b[38;5;241m.\u001b[39mfput_object(minio_bucket, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommentoxic/y_train\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/y_train.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/minio/api.py:989\u001b[0m, in \u001b[0;36mMinio.fput_object\u001b[0;34m(self, bucket_name, object_name, file_path, content_type, metadata, sse, progress, part_size, num_parallel_uploads, tags, retention, legal_hold)\u001b[0m\n\u001b[1;32m    987\u001b[0m file_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(file_path)\u001b[38;5;241m.\u001b[39mst_size\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_data:\n\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpart_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpart_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_uploads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_uploads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegal_hold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegal_hold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/minio/api.py:1766\u001b[0m, in \u001b[0;36mMinio.put_object\u001b[0;34m(self, bucket_name, object_name, data, length, content_type, metadata, sse, progress, part_size, num_parallel_uploads, tags, retention, legal_hold)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m upload_id:\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_abort_multipart_upload(\n\u001b[1;32m   1764\u001b[0m         bucket_name, object_name, upload_id,\n\u001b[1;32m   1765\u001b[0m     )\n\u001b[0;32m-> 1766\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/minio/api.py:1750\u001b[0m, in \u001b[0;36mMinio.put_object\u001b[0;34m(self, bucket_name, object_name, data, length, content_type, metadata, sse, progress, part_size, num_parallel_uploads, tags, retention, legal_hold)\u001b[0m\n\u001b[1;32m   1747\u001b[0m             part_number, etag \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m   1748\u001b[0m             parts[part_number\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m Part(part_number, etag)\n\u001b[0;32m-> 1750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_complete_multipart_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ObjectWriteResult(\n\u001b[1;32m   1754\u001b[0m         result\u001b[38;5;241m.\u001b[39mbucket_name,\n\u001b[1;32m   1755\u001b[0m         result\u001b[38;5;241m.\u001b[39mobject_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1759\u001b[0m         location\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mlocation,\n\u001b[1;32m   1760\u001b[0m     )\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/minio/api.py:1548\u001b[0m, in \u001b[0;36mMinio._complete_multipart_upload\u001b[0;34m(self, bucket_name, object_name, upload_id, parts)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     SubElement(tag, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m part\u001b[38;5;241m.\u001b[39metag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1547\u001b[0m body \u001b[38;5;241m=\u001b[39m getbytes(element)\n\u001b[0;32m-> 1548\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/xml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-MD5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5sum_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muploadId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompleteMultipartUploadResult(response)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/minio/api.py:403\u001b[0m, in \u001b[0;36mMinio._execute\u001b[0;34m(self, method, bucket_name, object_name, body, headers, query_params, preload_content, no_body_trace)\u001b[0m\n\u001b[1;32m    400\u001b[0m region \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_region(bucket_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_body_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_body_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3Error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryHead\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/minio/api.py:386\u001b[0m, in \u001b[0;36mMinio._url_open\u001b[0;34m(self, method, region, bucket_name, object_name, body, headers, query_params, preload_content, no_body_trace)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_error\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoSuchBucket\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryHead\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_region_map\u001b[38;5;241m.\u001b[39mpop(bucket_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 386\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m response_error\n",
      "\u001b[0;31mS3Error\u001b[0m: S3 operation failed; code: InvalidPart, message: One or more of the specified parts could not be found.  The part may not have been uploaded, or the specified entity tag may not match the part's entity tag., resource: /mlpipeline/commentoxic/x_train, request_id: 1745CACBF8162ABC, host_id: 6810e5c8-b975-4ef0-b5bb-49a4f57b0a8c, bucket_name: mlpipeline, object_name: commentoxic/x_train"
     ]
    }
   ],
   "source": [
    "transform_and_split_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fea3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
